<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Gaussian Copula Imputer" href="2.2_gaussian_copula_tutorial.html" /><link rel="prev" title="Threshold KNN‑Shapley Explainer" href="1.2_threshold_KNN_tutorial.html" />
        <link rel="prefetch" href="../_static/SEP_Logo.png" as="image" />

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.07.19 -->
        <title>Gaussian Conditional Imputer - [SEP_SoSe2025 / Gruppe C] Game Theoretic Explainable Artificial Intelligence</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">[SEP_SoSe2025 / Gruppe C] Game Theoretic Explainable Artificial Intelligence</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/SEP_Logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">[SEP_SoSe2025 / Gruppe C] Game Theoretic Explainable Artificial Intelligence</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../modules.html">shapiq_student</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of shapiq_student</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../shapiq_student.html">shapiq_student package</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="1.1_1.3_basic_and_weighted_KNN_tutorial.html">KNN-Shapley Explainer Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="1.2_threshold_KNN_tutorial.html">Threshold KNN‑Shapley Explainer</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Gaussian Conditional Imputer</a></li>
<li class="toctree-l1"><a class="reference internal" href="2.2_gaussian_copula_tutorial.html">Gaussian Copula Imputer</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_subset_finding_tutorial.html">Greedy Coalition Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_subset_finding_tutorial.html#beam-search-coalition-method">Beam Search Coalition Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_subset_finding_tutorial.html#beam-search-coalition-method-with-specified-beam-width">Beam Search Coalition Method with specified beam_width</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_subset_finding_tutorial.html#recursive-greedy-algorithm">Recursive greedy algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_Laufzeitanalyse_shapiq_shap.html">Laufzeitanalyse Shapiq vs Shap</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/notebooks/2.1_gaussian_imputer_tutorial.ipynb.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="gaussian-conditional-imputer">
<h1>Gaussian Conditional Imputer<a class="headerlink" href="#gaussian-conditional-imputer" title="Link to this heading">¶</a></h1>
<p>Willkommen zu diesem Tutorial über den Gaussian Conditional Imputer!
Hier zeigen wir, wie man <strong>Guassian Conditional Imputer</strong> in Python implementiert und anwendet.</p>
<section id="inhaltsverzeichnis">
<h2>Inhaltsverzeichnis<a class="headerlink" href="#inhaltsverzeichnis" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Was ist ein Gaussian Conditional Imputer?</p></li>
<li><p>Warum brauchen wir einen Gaussian Conditional Imputer?</p></li>
<li><p>Wie sieht die Implementierung aus?</p></li>
<li><p>Test und Visualisierung</p></li>
<li><p>Grenzen und Verbesserungsmöglichkeiten</p></li>
<li><p>Fazit</p></li>
</ol>
</section>
<section id="was-ist-ein-gaussian-conditional-imputer">
<h2>1. Was ist ein Gaussian Conditional Imputer?<a class="headerlink" href="#was-ist-ein-gaussian-conditional-imputer" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Ziel: Fehlende Feature-Werte realistisch ergänzen (= <em>Imputation</em>)</p></li>
<li><p>Annahme: Die Daten folgen einer <strong>multivariaten Normalverteilung</strong></p></li>
<li><p>Grundidee: Wenn ein Teil der Features bekannt ist → andere aus der bedingten Verteilung berechnen</p></li>
<li><p>Formeln: basiert auf Gleichung (10) und (11) aus <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0004370221000539?via%3Dihub#fm0190">Aas et al. (2021)</a>, die die <strong>bedingte Erwartung</strong> und die <strong>bedingte Konvarianzmatrix</strong> liefern</p></li>
</ul>
</section>
<section id="warum-brauchen-wir-einen-gaussian-conditional-imputer">
<h2>2. Warum brauchen wir einen Gaussian Conditional Imputer?<a class="headerlink" href="#warum-brauchen-wir-einen-gaussian-conditional-imputer" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Problem: Viele klassische Methoden (z.B. shap) nehmen Unabhängigkeit zwischen Features an
→ Fehlende Features werden zufällig oder mit Mittelwert ersetzt</p></li>
<li><p>Realität: Features sind oft <strong>korreliert</strong> (z.B. Einkommen und Bildung)<br />
→ Unabhängigkeitsannahme verzerrt die Shapley-Werte</p></li>
<li><p>Lösung mit Gaussian Conditional Imputer:
- Nutzt die bedingte multivariate Normalverteilung
- Sampelt realistische Werte, basierend auf den bekannten Features
- Führt zu präziseren, konsistenteren Shapley-Werten</p></li>
</ul>
</section>
<section id="wie-sieht-die-implementierung-aus">
<h2>3. Wie sieht die Implementierung aus?<a class="headerlink" href="#wie-sieht-die-implementierung-aus" title="Link to this heading">¶</a></h2>
<p>In diesem Abschnitt analysieren wir den Code <code class="docutils literal notranslate"><span class="pre">GaussianImputer</span></code>-Klasse Schritt für Schritt. Die Klasse erweitert das Imputer-Interface aus <code class="docutils literal notranslate"><span class="pre">shapiq</span></code> und verwendet die multivariate Normalverteilung, um fehlende Features zu ergänzen.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Überprüfen der shapiq-Version</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>shapiq<span class="w"> </span>overrides<span class="w"> </span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">shapiq</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shapiq version: </span><span class="si">{</span><span class="n">shapiq</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>shapiq version: 1.3.1
</pre></div>
</div>
</div>
</div>
<section id="imports-und-uberblick">
<h3>3.1 Imports und Überblick<a class="headerlink" href="#imports-und-uberblick" title="Link to this heading">¶</a></h3>
<p>Zuerst importieren wir die benötigten Pakete und bereiten die Klasse <code class="docutils literal notranslate"><span class="pre">GaussianImputer</span></code> vor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">shapiq.games.imputer.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">Imputer</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="konstruktur-init">
<h3>3.2 Konstruktur (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)<a class="headerlink" href="#konstruktur-init" title="Link to this heading">¶</a></h3>
<p>Die Klasse erbt von der abstrakten <code class="docutils literal notranslate"><span class="pre">Imputer</span></code>-Klasse im <code class="docutils literal notranslate"><span class="pre">shapiq</span></code>-Framework.<br />
Die Methode <code class="docutils literal notranslate"><span class="pre">__init__</span></code> initialisiert das Objekt mit Modell, Daten, dem zu erklärenden Punkt <code class="docutils literal notranslate"><span class="pre">x</span></code> und weiteren Parametern.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GaussianImputer</span><span class="p">(</span><span class="n">Imputer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">object</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
        <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">sample_size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cond_idx</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cond_values</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="fit-methode">
<h3>3.3 Fit-Methode<a class="headerlink" href="#fit-methode" title="Link to this heading">¶</a></h3>
<p>Diese Methode speichert den Datenpunkt <code class="docutils literal notranslate"><span class="pre">x</span></code>, den wir erklären wollen.<br />
Dabei werden vorherige Zustände (<code class="docutils literal notranslate"><span class="pre">cond_idx</span></code>, <code class="docutils literal notranslate"><span class="pre">cond_values</span></code>) zurückgesetzt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GaussianImputer</span><span class="p">:</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cond_idx</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cond_values</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="hauptlogik-call">
<h3>3.4 Hauptlogik: <code class="docutils literal notranslate"><span class="pre">__call__()</span></code><a class="headerlink" href="#hauptlogik-call" title="Link to this heading">¶</a></h3>
<p>Diese Methode berechnet für jede gegebene Koalition (Subset von Features) eine erwartete Modellvorhersage. Dabei wird angenommen, dass manche Features (<code class="docutils literal notranslate"><span class="pre">1</span></code>) und andere unbekannt (<code class="docutils literal notranslate"><span class="pre">0</span></code>) sind.</p>
<p>Ablauf:</p>
<ol class="arabic simple">
<li><p>Bekannte Features (<code class="docutils literal notranslate"><span class="pre">1</span></code>) bleiben unverändert</p></li>
<li><p>Unbekannte Features (<code class="docutils literal notranslate"><span class="pre">0</span></code>) werden mithilfe der bedingten multivariaten Normalverteilung ergänzt</p></li>
<li><p>Die vervollständigten Datenpuntkte werden dem Modell übergeben</p></li>
<li><p>Das Modell sagt Vorhersagen für alle Stichproben voraus</p></li>
<li><p>Der empirische Mittelwert dieser Vorhersagen wird zurückgegeben</p></li>
</ol>
<p>-&gt; Damit approximiert diese Methode die Shapley-Funktionsauswertung
<span class="math notranslate nohighlight">\( v(S) = E[f(x) \mid x_s] \)</span>, welche exakt auf Gleichung (2) aus Aas et al. (2021) basiert.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coalitions</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">coalition</span> <span class="ow">in</span> <span class="n">coalitions</span><span class="p">:</span>
        <span class="c1"># 1: Indizes der bekannten (cond_idx) und fehlenden (target_idx) Features ermitteln</span>
        <span class="n">cond_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">coalition</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_players</span><span class="p">),</span> <span class="n">cond_idx</span><span class="p">)</span>

        <span class="c1"># Spezialfälle: leere Koalition oder alle Features bekannt</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cond_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">empty_prediction</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="c1"># 2: Werte der bekannten Features extrahieren</span>
        <span class="n">cond_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">]</span>

        <span class="c1"># 3: Bedingte Samples aus der multivariaten Normalverteilung ziehen</span>
        <span class="n">samples</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_conditional_gaussian</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
            <span class="n">cond_idx</span><span class="p">,</span>
            <span class="n">cond_values</span><span class="p">,</span>
            <span class="n">n_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_size</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># 4: Vorhersage mit dem Modell für alle Samples</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 5: Mittelwert der Vorhersagen speichern</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_pred</span><span class="p">)</span>

    <span class="c1"># 6: Ergebnisse als Numpy-Array zurückgeben</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="der-kern-sample-conditional-gaussian">
<h3>3.5 Der Kern: <code class="docutils literal notranslate"><span class="pre">sample_conditional_gaussian()</span></code><a class="headerlink" href="#der-kern-sample-conditional-gaussian" title="Link to this heading">¶</a></h3>
<p>In dieser Methode wird die zentrale mathematische Idee des Gaussian Imputers umgesetzt. Wir berechnen die bedingte Verteilung der unbekannten Features
<span class="math notranslate nohighlight">\(x_{ \bar {S}}\)</span>, gegeben die bekannten Features <span class="math notranslate nohighlight">\(x_{{S}}\)</span>. Ziel ist es, Stichproben für die fehlenden Features zu generieren.</p>
<section id="schritt-1-erwartungswert-und-kovarianz-der-gesamtdaten-berechnen">
<h4>▶ Schritt 1: Erwartungswert und Kovarianz der Gesamtdaten berechnen<a class="headerlink" href="#schritt-1-erwartungswert-und-kovarianz-der-gesamtdaten-berechnen" title="Link to this heading">¶</a></h4>
<p>Wir gehen davon aus, dass alle Daten aus einer multivariaten Normalverteilung stammen.
Daher berechnen wir:</p>
<ul class="simple">
<li><p>den Erwartungswert <span class="math notranslate nohighlight">\( \mu \)</span></p></li>
<li><p>die Kovarianzmatrix <span class="math notranslate nohighlight">\( \Sigma \)</span></p></li>
</ul>
</section>
<section id="schritt-2-aufteilen-in-bekannte-s-und-unbekannte-bar-s-features">
<h4>▶ Schritt 2: Aufteilen in bekannte (<span class="math notranslate nohighlight">\(S\)</span>) und unbekannte (<span class="math notranslate nohighlight">\(\bar{S}\)</span>) Features<a class="headerlink" href="#schritt-2-aufteilen-in-bekannte-s-und-unbekannte-bar-s-features" title="Link to this heading">¶</a></h4>
<p>Indizes werden entsprechend getrennt.
Danach teilen wir auch den Erwartungswert <span class="math notranslate nohighlight">\( \mu \)</span> und die Kovarianzmatrix <span class="math notranslate nohighlight">\( \Sigma \)</span> in Teilmatrizen auf:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \mu_S \)</span>, <span class="math notranslate nohighlight">\( \mu_{\bar{S}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \Sigma_{SS} \)</span>, <span class="math notranslate nohighlight">\( \Sigma_{\bar{S}\bar{S}} \)</span>, <span class="math notranslate nohighlight">\( \Sigma_{\bar{S}S} \)</span>, <span class="math notranslate nohighlight">\( \Sigma_{S\bar{S}} \)</span></p></li>
</ul>
</section>
<section id="schritt-3-berechnung-der-bedingten-verteilung">
<h4>▶ Schritt 3: Berechnung der bedingten Verteilung<a class="headerlink" href="#schritt-3-berechnung-der-bedingten-verteilung" title="Link to this heading">¶</a></h4>
<p>Die Formeln stammen aus <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0004370221000539?via%3Dihub#fm0190">Aas et al. (2021)</a>:</p>
<ul class="simple">
<li><p><strong>Gleichung (10)</strong>: Bedingter Erwartungswert
<span class="math notranslate nohighlight">\(  \mu_{\bar{S} \mid S} = \mu_{\bar{S}} + \Sigma_{\bar{S}S} \Sigma_{SS}^{-1} (x_S^* - \mu_S) \)</span></p></li>
<li><p><strong>Gleichung (11)</strong>: Bedingte Kovarianzmatrix
<span class="math notranslate nohighlight">\(  \Sigma_{\bar{S} \mid S} = \Sigma_{\bar{S}\bar{S}} - \Sigma_{\bar{S}S} \Sigma_{SS}^{-1} \Sigma_{S\bar{S}} \)</span></p></li>
</ul>
<p>Hinweis: Wir verwenden die Pseudoinserse (<code class="docutils literal notranslate"><span class="pre">np.linalg.pinv</span></code>), falls die Matrix nicht invertierbar ist.</p>
</section>
<section id="schritt-4-ziehen-von-stichproben">
<h4>▶ Schritt 4: Ziehen von Stichproben<a class="headerlink" href="#schritt-4-ziehen-von-stichproben" title="Link to this heading">¶</a></h4>
<p>Nun erzeugen wir Stichproben aus
<span class="math notranslate nohighlight">\( N(\mu_{\bar{S}|S}, \Sigma_{\bar{S}|S})
\)</span>
Diese Samples repräsentieren plausible Werte für die fehlenden Features.</p>
</section>
<section id="ergebnis">
<h4>▶ Ergebnis:<a class="headerlink" href="#ergebnis" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">conditional_samples</span></code>: Die gesampelten Werte der fehlenden Features</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mu_cond</span></code>: Der berechnete Erwartungswert der bedingten Verteilung</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sigma_cond</span></code>: Die zugehörige Kovarianzmatrix</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_conditional_gaussian</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">cond_idx</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">cond_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asanyarray</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">cond_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asanyarray</span><span class="p">(</span><span class="n">cond_values</span><span class="p">)</span>

    <span class="c1"># 1: Erwartungswert und Kovarianzmatrix</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sigma_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># 2: Aufteilen in bekannte (S) und unbekannte (S̄) Features</span>
    <span class="n">feature_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">target_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">feature_indices</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">,</span> <span class="n">assume_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">mu_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">)</span>  <span class="c1"># μ_S</span>
    <span class="n">mu_sbar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">target_idx</span><span class="p">)</span>  <span class="c1"># μ_{S̄}</span>

    <span class="c1"># Teilmatrizen aus der Kovarianzmatrix extrahieren</span>
    <span class="n">sigma_ss</span> <span class="o">=</span> <span class="n">sigma_full</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">cond_idx</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">)]</span>  <span class="c1"># Σ_{SS}</span>
    <span class="n">sigma_sbar_sbar</span> <span class="o">=</span> <span class="n">sigma_full</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">target_idx</span><span class="p">,</span> <span class="n">target_idx</span><span class="p">)]</span>  <span class="c1"># Σ_{S̄S̄}</span>
    <span class="n">sigma_sbar_s</span> <span class="o">=</span> <span class="n">sigma_full</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">target_idx</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">)]</span>  <span class="c1"># Σ_{S̄S}</span>
    <span class="n">sigma_s_sbar</span> <span class="o">=</span> <span class="n">sigma_sbar_s</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Σ_{SS̄}</span>

    <span class="c1"># 3: Berechnung der bedingten Verteilung</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">cond_values</span> <span class="o">-</span> <span class="n">mu_s</span>
    <span class="n">inv_sigma_ss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">sigma_ss</span><span class="p">)</span>
    <span class="n">mu_cond</span> <span class="o">=</span> <span class="n">mu_sbar</span> <span class="o">+</span> <span class="n">sigma_sbar_s</span> <span class="o">@</span> <span class="n">inv_sigma_ss</span> <span class="o">@</span> <span class="n">delta</span>
    <span class="n">sigma_cond</span> <span class="o">=</span> <span class="n">sigma_sbar_sbar</span> <span class="o">-</span> <span class="n">sigma_sbar_s</span> <span class="o">@</span> <span class="n">inv_sigma_ss</span> <span class="o">@</span> <span class="n">sigma_s_sbar</span>

    <span class="c1"># 4: Ziehen von Stichproben aus der bedingten multivariaten Normalverteilung</span>
    <span class="n">conditional_samples</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="n">mu_cond</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">sigma_cond</span><span class="p">,</span> <span class="n">allow_singular</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># Sicherstellen, dass die Ausgabe 2D ist</span>
    <span class="k">if</span> <span class="n">conditional_samples</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">conditional_samples</span> <span class="o">=</span> <span class="n">conditional_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">conditional_samples</span><span class="p">,</span> <span class="n">mu_cond</span><span class="p">,</span> <span class="n">sigma_cond</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="test-und-visualisierung">
<h2>4. Test und Visualisierung<a class="headerlink" href="#test-und-visualisierung" title="Link to this heading">¶</a></h2>
<section id="test-mit-kunstlichen-testdaten">
<h3>4.1 Test mit künstlichen Testdaten<a class="headerlink" href="#test-mit-kunstlichen-testdaten" title="Link to this heading">¶</a></h3>
<p>In diesem Abschnitt testen wir unsere Hauptmethode <code class="docutils literal notranslate"><span class="pre">sample_conditional_gaussian()</span></code> mit künstlich generierten Daten.</p>
<ul class="simple">
<li><p>Die multivariate Verteilung hat einen bekannten Erwartungswert und eine Kovarianzmatrix</p></li>
<li><p>Wir fixieren ein Feature und berechnen die bedingte Verteilung der restlichen Features</p></li>
<li><p>Dann vergleichen wir:</p>
<ul>
<li><p>empirisch berechneten Mittelwert der gesampelten Daten</p></li>
<li><p>mit dem theoretischen Erwartungswert</p></li>
</ul>
</li>
<li><p>Wenn die beide Werte (und auch die Kovarianzmatrix) gut übereinstimmen, zeigt das, dass unsere Implementierung korrekt ist.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.patches</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ellipse</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dummy-Modell für den Test</span>
<span class="k">def</span><span class="w"> </span><span class="nf">dummy_model</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">test_sample_conditional_gaussian</span><span class="p">():</span>
    <span class="c1"># Testdaten</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="n">sample_data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

    <span class="c1"># Index und Wert der bekannten Features</span>
    <span class="n">cond_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cond_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="c1"># GaussianImputer mit Dummy-Modell initialisieren</span>
    <span class="n">imputer</span> <span class="o">=</span> <span class="n">GaussianImputer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">dummy_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">sample_data</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Bedingte Samples erzeugen / Erwartungswert + Kovarianz berechnen</span>
    <span class="n">samples</span><span class="p">,</span> <span class="n">mu_cond</span><span class="p">,</span> <span class="n">sigma_cond</span> <span class="o">=</span> <span class="n">sample_conditional_gaussian</span><span class="p">(</span>
        <span class="n">sample_data</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">,</span> <span class="n">cond_values</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span>
    <span class="p">)</span>

    <span class="c1"># Empirische Werte berechnen</span>
    <span class="n">empirical_mean</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">empirical_cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Vergleiche mit den theoretischen Werten</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">empirical_mean</span><span class="p">,</span> <span class="n">mu_cond</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">empirical_cov</span><span class="p">,</span> <span class="n">sigma_cond</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bedingte Erwartungswerte:&quot;</span><span class="p">,</span> <span class="n">mu_cond</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Empirischer Mittelwert:&quot;</span><span class="p">,</span> <span class="n">empirical_mean</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&gt; Differnez:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">mu_cond</span> <span class="o">-</span> <span class="n">empirical_mean</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bedingte Kovarianzmatrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">sigma_cond</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Empirische Kovarianzmatrix</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">empirical_cov</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&gt; Differenz:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">sigma_cond</span> <span class="o">-</span> <span class="n">empirical_cov</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Conditional Samples:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>


<span class="n">test_sample_conditional_gaussian</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Bedingte Erwartungswerte: [0.40247896 0.24290748]
Empirischer Mittelwert: [0.39989483 0.25864618]
-&gt; Differnez: [0.00258413 0.0157387 ]

Bedingte Kovarianzmatrix:
 [[ 0.3634229  -0.08905025]
 [-0.08905025  0.73043497]]
Empirische Kovarianzmatrix
 [[ 0.35570637 -0.07859925]
 [-0.07859925  0.74182342]]
-&gt; Differenz:
 [[0.00771653 0.010451  ]
 [0.010451   0.01138845]]

Conditional Samples:
 [[ 1.1824208  -0.54311823]
 [-0.51215731  0.28431153]
 [ 1.4573317  -0.02911964]
 ...
 [-0.30062915  0.82558158]
 [-0.34799363 -0.15950655]
 [ 0.41051004  1.17349108]]
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong>Interpretation</strong></p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">Bedingte</span> <span class="pre">Erwartungswerte</span></code></p>
<ul class="simple">
<li><p>Feature 0 = 0.5 -&gt; erwartete Durchschnittswerte für Feature 1 und 2</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Bedingte</span> <span class="pre">Kovarianzmatrix</span></code></p>
<ul class="simple">
<li><p>zwischen Feature 1 und 2</p></li>
<li><p>Gültig unter Bedingung: Feature 0 =  0.5</p></li>
<li><p>Zeigt Richtung und Stärke der Abhängigkeit zwischen den verbleibenden Features</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Conditional</span> <span class="pre">Samples</span></code></p>
<ul class="simple">
<li><p>Gesampelte Werte aus der bedingten Normalverteilung</p></li>
<li><p>Realistische Kombinationen von Feature 1 und 2 bei fixiertem Feature 0</p></li>
</ul>
<p>▶ Empirische Werte ≈ Theoretische Werte<br />
▶ Implementierung korrekt!</p>
</section>
<section id="visualisierung">
<h3>4.2 Visualisierung<a class="headerlink" href="#visualisierung" title="Link to this heading">¶</a></h3>
<p>Diese Visualisierung überprüft:</p>
<ol class="arabic simple">
<li><p>Ob die Stichproben wirklich der theoretischen bedingten Verteilung entsprechen</p></li>
<li><p>In der Praxis wird dies durch numerische Tests überprüft, z.B:</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Testdaten und Stichproben erzeugen</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="n">sample_data</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">cond_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cond_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">samples</span><span class="p">,</span> <span class="n">mu_cond</span><span class="p">,</span> <span class="n">sigma_cond</span> <span class="o">=</span> <span class="n">sample_conditional_gaussian</span><span class="p">(</span>
    <span class="n">sample_data</span><span class="p">,</span> <span class="n">cond_idx</span><span class="p">,</span> <span class="n">cond_values</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>

<span class="c1"># Datenframe erzeugen</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Feature 2&quot;</span><span class="p">])</span>

<span class="c1"># Stichproben als Streudiagramm</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Bedingte Samples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
    <span class="n">mu_cond</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu_cond</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Bedingter Erwartungswert&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>

<span class="c1"># 95%-Kofidenzellipse aus der bedingten Kovarianz</span>
<span class="n">vals</span><span class="p">,</span> <span class="n">vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">sigma_cond</span><span class="p">)</span>
<span class="n">order</span> <span class="o">=</span> <span class="n">vals</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">vals</span> <span class="o">=</span> <span class="n">vals</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">vecs</span><span class="p">[:,</span> <span class="n">order</span><span class="p">]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="o">*</span><span class="n">vecs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>

<span class="n">ellipse</span> <span class="o">=</span> <span class="n">Ellipse</span><span class="p">(</span>
    <span class="n">xy</span><span class="o">=</span><span class="n">mu_cond</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
    <span class="n">angle</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>
    <span class="n">fc</span><span class="o">=</span><span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">%-E</span><span class="s2">llipse der Kovarianz&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>

<span class="c1"># Achsenbeschriftung, Titel, Legende, Layout</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Bedingte Verteilung von Feature 1 &amp; 2 | Feature 0 = 0.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7ef493ce0ddc2b641d7238ccc0cd73e93dd277c067a93001a02f27b926aa06f0.png" src="../_images/7ef493ce0ddc2b641d7238ccc0cd73e93dd277c067a93001a02f27b926aa06f0.png" />
</div>
</div>
<blockquote>
<div><p><strong>Interpretation</strong></p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">Bedingte</span> <span class="pre">Samples</span></code></p>
<ul class="simple">
<li><p>Stichproben aus der bedingten Verteilung von Feature 1 und 2</p></li>
<li><p>gegeben Feature 0 = 0.5</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Bedingter</span> <span class="pre">Erwartungswert</span></code></p>
<ul class="simple">
<li><p>Theoretisch berechneter bedingter Erwartungswert (<em>mu_cond</em>)</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">95%-Ellipse</span> <span class="pre">der</span> <span class="pre">Kovarianz</span></code></p>
<ul class="simple">
<li><p>95%-Konfidenzregion, basierend auf der bedingten Kovarianzmatrix (<em>sigma_cond</em>)</p></li>
</ul>
<p>◼ Die Stichproben liegen gut innerhalb der Ellipse (basieren auf der Kovarianzstruktur)<br />
→ Die bedingte Kovarianz wurde unter der Gaußschen Annahme korrekt berücksichtigt.</p>
<p>◼ Die rote Mittelpunkt liegt nahe am Zentrum der Stichprobenverteilung.<br />
→ Der Imputer hat den bedingten Erwartungswert korrekt geschätzt.</p>
</section>
</section>
<section id="grenzen-und-verbesserungsmoglichkeiten">
<h2>5. Grenzen und Verbesserungsmöglichkeiten<a class="headerlink" href="#grenzen-und-verbesserungsmoglichkeiten" title="Link to this heading">¶</a></h2>
<p>Obwohl der Gaussian Conditional Imputer bei der Apporoximation von bedingten Verteilungen nützlich ist, hat er einige wichtige Einschränkungen:</p>
<blockquote>
<div><p>Starke Annahme der multivariate Normalverteilung</p>
</div></blockquote>
<p>Diese Annahme ist in realen Datensätzen selten erfüllt, insbesondere wenn die Verteilungen schief oder komplex sind.</p>
<blockquote>
<div><p>Fehlende Flexibilität bei nicht-linearen Abhängigkeiten</p>
</div></blockquote>
<p>Wenn die Abhängigkeiten nicht-linear oder asymmetrisch sind, ist das Modell nicht realistisch.</p>
<p>Um diese Einschränkungen zu überwinden, schlägt Aas et al. (2021) vor, die Feature-Abhängigkeiten explizit zu modellieren, ohne sich auf Normalverteilung zu verlassen. Die <strong>Gaussian Copula</strong> erlaubt es, die lineare Abhängigkeit beizubehalten, während gleichzeitig beliebige Marginalverteilungen (d.h. Form der einzelnen Features) erlaubt sind.</p>
</section>
<section id="fazit">
<h2>6. Fazit<a class="headerlink" href="#fazit" title="Link to this heading">¶</a></h2>
<p>Der <strong>Gaussian Conditional Imputer</strong> bietet uns ein einfaches und klares Paradigma, um fehlende Werte unter Annahme einer Normalverteilung zu ergänzen. Diese Methode funktioniert gut, wenn die Daten wirklich annährend normalverteilt sind.
In der Praxis sehen wir jedoch oft nicht-lineare Abhängigkeiten oder asymmetrische Verteilungen, die der Gaussian Imputer nicht gut abbilden kann.</p>
<p>Genau hier bietet der <strong>Gaussian Copula Conditional Imputer</strong> eine wichtige Verbesserung, weil er mehr Flexibilität bei der Modellierung von Abhängigkeiten und Randverteilungen ermöglicht.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="2.2_gaussian_copula_tutorial.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Gaussian Copula Imputer</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="1.2_threshold_KNN_tutorial.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Threshold KNN‑Shapley Explainer</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Gaussian Conditional Imputer</a><ul>
<li><a class="reference internal" href="#inhaltsverzeichnis">Inhaltsverzeichnis</a></li>
<li><a class="reference internal" href="#was-ist-ein-gaussian-conditional-imputer">1. Was ist ein Gaussian Conditional Imputer?</a></li>
<li><a class="reference internal" href="#warum-brauchen-wir-einen-gaussian-conditional-imputer">2. Warum brauchen wir einen Gaussian Conditional Imputer?</a></li>
<li><a class="reference internal" href="#wie-sieht-die-implementierung-aus">3. Wie sieht die Implementierung aus?</a><ul>
<li><a class="reference internal" href="#imports-und-uberblick">3.1 Imports und Überblick</a></li>
<li><a class="reference internal" href="#konstruktur-init">3.2 Konstruktur (<code class="docutils literal notranslate"><span class="pre">__init__</span></code>)</a></li>
<li><a class="reference internal" href="#fit-methode">3.3 Fit-Methode</a></li>
<li><a class="reference internal" href="#hauptlogik-call">3.4 Hauptlogik: <code class="docutils literal notranslate"><span class="pre">__call__()</span></code></a></li>
<li><a class="reference internal" href="#der-kern-sample-conditional-gaussian">3.5 Der Kern: <code class="docutils literal notranslate"><span class="pre">sample_conditional_gaussian()</span></code></a><ul>
<li><a class="reference internal" href="#schritt-1-erwartungswert-und-kovarianz-der-gesamtdaten-berechnen">▶ Schritt 1: Erwartungswert und Kovarianz der Gesamtdaten berechnen</a></li>
<li><a class="reference internal" href="#schritt-2-aufteilen-in-bekannte-s-und-unbekannte-bar-s-features">▶ Schritt 2: Aufteilen in bekannte (<span class="math notranslate nohighlight">\(S\)</span>) und unbekannte (<span class="math notranslate nohighlight">\(\bar{S}\)</span>) Features</a></li>
<li><a class="reference internal" href="#schritt-3-berechnung-der-bedingten-verteilung">▶ Schritt 3: Berechnung der bedingten Verteilung</a></li>
<li><a class="reference internal" href="#schritt-4-ziehen-von-stichproben">▶ Schritt 4: Ziehen von Stichproben</a></li>
<li><a class="reference internal" href="#ergebnis">▶ Ergebnis:</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#test-und-visualisierung">4. Test und Visualisierung</a><ul>
<li><a class="reference internal" href="#test-mit-kunstlichen-testdaten">4.1 Test mit künstlichen Testdaten</a></li>
<li><a class="reference internal" href="#visualisierung">4.2 Visualisierung</a></li>
</ul>
</li>
<li><a class="reference internal" href="#grenzen-und-verbesserungsmoglichkeiten">5. Grenzen und Verbesserungsmöglichkeiten</a></li>
<li><a class="reference internal" href="#fazit">6. Fazit</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>